{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98af7301-8135-4444-9dba-8b8d7304645a",
   "metadata": {},
   "source": [
    "# Deep learning for natural language processing\n",
    "\n",
    "In previous practical works, you have learned the basics of Python programming, how to process a textual dataset, how to extract features such as TF-IDF, and how to train different machine learning models, including SVMs and MLPs (feed-forward neural networks), using the sklearn package. However, the sklearn package does not allow you to design and train custom neural networks in more detail. Therefore, here you will learn how to use pytorch to define, train and evaluate your own neural network model for different tasks. You will also learn the basics of deep learning and how to use pre-trained deep representations of data, such as BERT models, instead of traditional TF-IDF functions. Below is a list of things you will learn in this practical work.\n",
    "\n",
    "- Training TF-IDFs using SVMs (from previous practical work)\n",
    "- Designing a neural network model with Pytorch\n",
    "- Training a Pytorch model\n",
    "- Plotting and analysing training vs development loss\n",
    "- Evaluation of a trained pytorch model\n",
    "- Using huggingface to load BERT models\n",
    "- Using BERT representations for training machine learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaead604-2d9b-49cc-aa14-a3d98609df59",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Before we start with the practical work, we need to install and import the packages we will be using. To do so, run the scripts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26003278-6dea-4974-bb63-cfb484946a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.24.1\n",
    "%pip install pandas==1.5.3\n",
    "%pip install -U scikit-learn==1.2.1\n",
    "%pip install scipy==1.10.0\n",
    "%pip install matplotlib==3.6.3\n",
    "%pip install torch==1.13.1\n",
    "%pip install transformers==4.26.1\n",
    "%pip install sacremoses # only for flaubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb2969-eb66-477d-9f01-f01dc339adaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from transformers import DistilBertTokenizer DistilBertModel\n",
    "from transformers import FlaubertTokenizer, FlaubertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc962152-194e-4c0d-b05d-98d7691827a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a useful function to see the progress when running a loop\n",
    "def printProgressBar (iteration: int, total: int, prefix = '', suffix = '', decimals = 1, length = \"fit\", fill = '█') -> None:\n",
    "    \"\"\"Prints a progress bar on the terminal\n",
    "    \"\"\"\n",
    "    if length==\"fit\":\n",
    "        rows, columns = os.popen('stty size', 'r').read().split() # checks how wide the terminal width is\n",
    "        length = int(columns) // 2\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    if iteration == total: # go to new line when the progress bar is finished\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be740f21-0116-4dd3-89b6-d0bece54fdcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining metrics (from previous work)\n",
    "\n",
    "Here, as the task is classification, we will work with `accuracy` and `Unweighted Average Recall (UAR)`. For more infromation please check the previous practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7092ce-885a-4151-a136-a3053bdde8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(outs:list, tars:list) -> float:\n",
    "    \"\"\"\n",
    "    Calculating and returning the accuracy between outputs (`outs`) and targets (`tars`),\n",
    "    where each one is a list of integers like [1,0,1,2,2,1,0,3], with each integer indicating the target label\n",
    "    \"\"\"\n",
    "    accuracy = np.mean([out==tar for out, tar in zip(outs, tars)])\n",
    "    return accuracy\n",
    "\n",
    "def UAR(outs:list, tars:list) -> float:\n",
    "    \"\"\"\n",
    "    Calculating and returning the unweighted average recall between outputs (`outs`) and targets (`tars`),\n",
    "    where each one is a list of integers like [1,0,1,2,2,1,0,3], with each integer indicating the target label\n",
    "    \"\"\"\n",
    "    tarsSet = list(set(tars))\n",
    "    corrects = {}\n",
    "    totals = {}\n",
    "    for i in tarsSet:\n",
    "        corrects[i] = 0\n",
    "        totals[i] = 0\n",
    "    for i, out in enumerate(outs):\n",
    "        tar = tars[i]\n",
    "        totals[tar] += 1\n",
    "        if out == tar: corrects[tar] += 1 # Counting when target and output match per each class \n",
    "    uar = 0\n",
    "    for i in tarsSet:\n",
    "        uar += corrects[i] / totals[i] # Calculating the accuracy per each class \n",
    "    uar = uar / len(tarsSet) # Calculating the average of accuracies per each class \n",
    "    return uar\n",
    "\n",
    "outputs = [1,1,1,1,0,1,1,1]\n",
    "targets = [0,0,1,1,1,1,1,1]\n",
    "print(\"Accuracy example:\", accuracy(outputs, targets))\n",
    "print(\"UAR example:\", UAR(outputs, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c49720-ca8e-4f44-acc9-c57b2acb9d12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Processing a textual dataset (from previous work)\n",
    "\n",
    "Here, we will learn how to load a textual dataset, and extract `tf-idf` features for machine learning (`tf-idf` was the subject of the previous practical work). Here, we will focus on a sentiment analysis dataset, called `Allociné`, which consists of reviews of French television series. These reviews can be positive (labelled `1`) or negative (labelled `0`). In this section, we do the following:\n",
    "\n",
    "- Download the `Allociné` corpus\n",
    "- Load the three partitions of `train`, `dev`, and `test` into memory\n",
    "- Extract `tf-idf` features for each partition\n",
    "- Read sentiment targets as numerical values, ready for machine learning usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fff859-d08d-41a5-a9a8-cb344c120917",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "You can run the scripts below to download the dataset. This dataset is the same as the previous practical work, so the related files can also be copied here and put under the directory `allocine` next to where this `TP.ipynb` file is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52188c82-382f-40e4-8ded-240f8c054e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_path = \"http://sentiment.nlproc.org/sentiment-dataset-fr.zip\"\n",
    "os.system(f\"wget {dl_path}\")\n",
    "os.system(f\"unzip ./sentiment-dataset-fr.zip -d ./allocine\")\n",
    "os.system(f\"rm ./sentiment-dataset-fr.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a777cb-b697-4cd0-9f06-2a5833d9f7b7",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "Run the scripts below to load the dataset into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e1b63-f0cf-485d-a4d2-9265fdd585b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_dataset(path):\n",
    "    train_path = os.path.join(path)\n",
    "    train_data = np.loadtxt(train_path, dtype='str', delimiter='\\t', skiprows=0)\n",
    "    return train_data\n",
    "\n",
    "# you might change the following lines according to where the files are stored in your system\n",
    "train_data = get_all_dataset(\"./allocine/fr/train.tsv\") \n",
    "dev_data = get_all_dataset(\"./allocine/fr/dev.tsv\") \n",
    "test_data = get_all_dataset(\"./allocine/fr/test.tsv\") \n",
    "\n",
    "print(\"An example of a comment in the train partition:\\n\", train_data[0,1], \"\\n\")\n",
    "print(\"An example of a comment in the dev partition:\\n\", dev_data[0,1], \"\\n\")\n",
    "print(\"An example of a comment in the test partition:\\n\", test_data[0,1], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a0010-9a58-4faa-8abf-f73e419d61e1",
   "metadata": {},
   "source": [
    "### Extracting features\n",
    "\n",
    "In the previous practical work, we learned about the statistical measure 'tf-idf', which can be used as a basic but useful feature for processing text. In this practical work, we would like to extract features for different partitions using `tf-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e3517-f234-4cc4-9121-ff11b694e13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vectorizer(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    _ = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer\n",
    "\n",
    "corpus = train_data[:,1]\n",
    "vectorizer = get_vectorizer(corpus)\n",
    "\n",
    "feats_train_text = vectorizer.transform(train_data[:,1]).toarray()\n",
    "tars_train_text  = [int(num) for num in train_data[:,0]]\n",
    "\n",
    "feats_dev_text = vectorizer.transform(dev_data[:,1]).toarray()\n",
    "tars_dev_text  = [int(num) for num in dev_data[:,0]]\n",
    "\n",
    "feats_test_text = vectorizer.transform(test_data[:,1]).toarray()\n",
    "tars_test_text  = [int(num) for num in test_data[:,0]]\n",
    "\n",
    "print(\"the first five tf-idf features for the train partition:\\n\", feats_train_text[:5])\n",
    "print(\"the first five targets for the train partition:\\n\", tars_train_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb00209c-ea66-4d5a-9992-49e17d113534",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training and testing an SVM model (from previous work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f30669-f75a-48bf-8ca8-39ee0e6e1a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, kernel='rbf')\n",
    "clf.fit(feats_train_text, tars_train_text)\n",
    "preds = clf.predict(feats_test_text)\n",
    "print(\"Accuracy:\", accuracy(preds, tars_test_text))\n",
    "print(\"UAR:\", UAR(preds, tars_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba8555-911f-438a-a283-ed4d151e0287",
   "metadata": {},
   "source": [
    "## Testing an SVM model with custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaefc67-2b0c-4dce-8d4b-b3088174bae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_inputs = [\n",
    "                 \"c'est trop bien!\",\n",
    "                 \"c'est pas intéressant\",\n",
    "                 \"je me suis bien amusé avec ce TP\",\n",
    "                 \"c'est pas cool ça!!!\",\n",
    "                ]\n",
    "feats = vectorizer.transform(custom_inputs).toarray()\n",
    "preds = clf.predict(feats)\n",
    "\n",
    "for p, pred in enumerate(preds):\n",
    "    label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(custom_inputs[p], \"->\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2e142-e20a-4420-93dd-4a23c5ace35e",
   "metadata": {},
   "source": [
    "## Artificial neural networks\n",
    "\n",
    "![NNs](https://d35fo82fjcw0y8.cloudfront.net/2019/04/08023413/Neural_Network_Brain_Mimic.jpeg)\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a type of machine learning technique, which is loosely based on the concept of biological neural networks in the human brain. Each artificial neuron, similar to the synapses and axons of a biological neuron, can be connected to other neurons to send or receive information. Artificial neurons are usually put together as groups, which are called neural layers. Multiple layers of ANN can then be cascaded together in different ways to model more complex tasks, in which case they are referred to as Deep Neural Networks (DNNs). This fact, in theory, gives DNNs universal approximator abilities, which means that with enough layers, DNNs can represent any function with high precision.\n",
    "\n",
    "### Fully connected layers\n",
    "\n",
    "The most basic form of artificial neural layers, are fully connected layers, where all the neurons of the first layer is connected to all the neurons of the next layers. To describe how fully connected layers work through mathematical notations, we can consider the input of each layer to be a numerical vector, that is transformed to a different vector, through a matrix multiplication, and usually followed by a non-linear function. This process can be written as followed:\n",
    "\n",
    "\\begin{equation}\n",
    "y = h(Wx + b)\n",
    "\\end{equation}\n",
    "\n",
    "where $x$ is the input vector, $W$ is the weight matrix, $b$  is the bias vector, which is there to off-set the linear matrix multiplication, and $h(.)$ is usually a non-linear function such as tangent hyperbolic or sigmoid, and $y$ is the output vector.\n",
    "\n",
    "### Training neural networks\n",
    "\n",
    "Backpropagation with Stochastic Gradient Descent (SGD)-based algorithms first calculates the gradient of a given loss function with respect to each of the weights in a neural network. It then uses the chain rule of calculus to iteratively compute the gradients of each layer in the network. The gradients can then be used to update the weights of the network to minimise a given loss function. This process for each layer can be described as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "W = W - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{L}$ is the loss function, $\\frac{\\partial \\mathcal{L}}{\\partial W}$ is the derivative of the loss with respect to a weight matrix $W$, and $\\alpha$ is the learning rate, which lets us control how fast or slow the weights are updated for each iteration. Through back-propagation, a fully connected layer can theoretically be iteratively trained to recognise the spatial structure of data.\n",
    "\n",
    "\n",
    "Adam optimiser is used here to train ANN models. Adam is a SGD-based optimisation method for updating the weights of neural layers based on a given loss function. Put simply, Adam expands SGD by using a weighted average of the gradients to converge faster, and also decays the gradients during training so that convergence moves towards the global minimum in the early stages of training, and then slows down the oscillations as it approaches it. Although it is still debated whether SGD generalises better in the long run Adam,  it is more or less agreed that Adam can have comarable or better results than SGD while converging faster\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "There already exists some famous machine learning frameworks that allow users to build and train their neural networks for different tasks. One such framework is [PyTorch](https://pytorch.org/) which has a Python interface and is easy to use. Here this package is used to train and evaluate a model for sentiment analysis from text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617bee8-9802-4e32-8883-f0b77389bed2",
   "metadata": {},
   "source": [
    "## Defining a pytorch model\n",
    "\n",
    "In Pytorch, in order to define your own model, you have to define a class that extends the `nn.Module` class of Pytorch. Then, you also need to override the `forward` function, which is responsible for the forward pass of the ANN model. Other functions such as backward can also be overrided, but it is not necessary as it is automatically calculated from the forward pass. In fact, pytorch works by creating graphs that can be tracked for backpropagation with `torch.autograd` engine ([see here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)). Below, you can see a simple example of a pytorch neural network model with two linear (feed-forward) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b172dc-ee51-4aa6-95dc-40062dec58e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model_pth(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feat_size,\n",
    "                 hidden_size=64, \n",
    "                 output_size=2, \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.lin1 = nn.Linear(feat_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.lin1(x)\n",
    "        output = self.lin2(output)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(0)\n",
    "input_example = torch.rand(1, 768) # just a random tensor as input to test the model\n",
    "model_pth = Model_pth(768) # defining an instance of the class as our model\n",
    "out = model_pth(input_example) # passing the input to the model, this will automatically go through the forward function\n",
    "print(\"example output:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16cff1-8a1b-4394-9973-7a4094c95a89",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "- Look at the example output printed above, what does each element in the printed tensor represent? what does the \"grad_fn\" do?\n",
    "- What does the `torch.manual_seed(0)` do? (try changing 0 to different numbers to get a hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa471b-41e9-4d86-8dbd-41428a325820",
   "metadata": {},
   "source": [
    "## Training a pytorch model\n",
    "\n",
    "Now that we learnt how to define an ANN model, let's see how to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85a160-f49a-4c24-957e-0700169c768b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Excercise 1\n",
    "\n",
    "As explained above, ANNs are practically trainable models that can approximate any function in a data-driven manner. In this exercise we want to see this in action. Here, the task is to first do a forward pass to compute the output, then compute the error based on a criterion, and then use the error to change the weights of the neural network to better fit the training data. Most of the code is already written below, you just need to change the `...` parts and then answer the question below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104f8ec-631d-4a26-b185-1f18ad1d7077",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Define an instance of the class Model_pth defined above\n",
    "print(\"model weights for lin1, before backward pass:\", model_pth.lin1.weight)\n",
    "model_pth = Model_pth(768)\n",
    "\n",
    "optimizer = optim.Adam(model_pth.parameters(), lr=0.001) # Defining Adam as our optimiser\n",
    "#optimizer.zero_grad() # zero the parameter gradients\n",
    "criterion = nn.CrossEntropyLoss() # Defining cross-entropy as our loss function\n",
    "input_example = torch.rand(1, 768) # just a random tensor as input to test the model\n",
    "label_example = torch.tensor([0]) # Let's imagine the target label of the input_example is `0`\n",
    "\n",
    "# Pass the input_example the model to output \n",
    "outputs = ...\n",
    "print(\"outputs before backward pass:\", outputs)\n",
    "\n",
    "# Calculate the loss between the output and the label_example\n",
    "loss = ...\n",
    "print(\"loss\", loss)\n",
    "\n",
    "# Backward the calculated loss through the model to change the gradients of each layer\n",
    "loss.backward()\n",
    "\n",
    "# Use the optimiser to update the weights based on the calculated gradients from the loss function\n",
    "optimizer.step()\n",
    "print(\"model weights for lin1, after backward pass:\", model_pth.lin1.weight)\n",
    "\n",
    "# Pass the input_example to the updated model\n",
    "outputs = model_pth(input_example)\n",
    "print(\"outputs after backward pass:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cece99-8faf-4411-9228-2f1ee0e3238e",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "- How do you think updating the weights as above has changed the model's predictions for the same `input_example` before and after the backward pass?\n",
    "\n",
    "- Try changing `label_example` from `torch.tensor([0])` to `torch.tensor([1])`, and then run the code above again, what do you observe in outputs after backward pass in both cases? why? (help: look for how \"CrossEntropyLoss\" and classification works for neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcca75d-4df1-483a-83bd-6a52ea402e8f",
   "metadata": {},
   "source": [
    "### Writing a fit function for pytroch\n",
    "\n",
    "In order to train the neural network model defined above, and the way we can train neural networks in PyTorch, here we write a function that can train different neural networks for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ed066-09a6-44fc-86da-accadeb7a811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_pth_model(max_epoch, \n",
    "                  model_pth, \n",
    "                  models_save_dir, \n",
    "                  feats_train, \n",
    "                  tars_train,\n",
    "                  feats_dev, \n",
    "                  tars_dev,\n",
    "                 ):\n",
    "    torch.manual_seed(0)\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    optimizer = optim.Adam(model_pth.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if not os.path.exists(models_save_dir): os.makedirs(models_save_dir)\n",
    "    for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "        train_loss = 0\n",
    "        model_pth.train()\n",
    "        for feats, tars in zip(feats_train, tars_train):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = torch.tensor(feats).unsqueeze(0).float()\n",
    "            labels = torch.tensor(tars).unsqueeze(0)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model_pth(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += round(loss.item(), 3)\n",
    "\n",
    "        train_loss /= len(tars_train)\n",
    "        train_losses.append(train_loss)\n",
    "        torch.save(model_pth, os.path.join(models_save_dir, f\"{epoch}.pth\"))\n",
    "\n",
    "        dev_loss = 0\n",
    "        model_pth.eval()\n",
    "        for feats, tars in zip(feats_dev, tars_dev):\n",
    "            inputs = torch.tensor(feats).unsqueeze(0).float()\n",
    "            labels = torch.tensor(tars).unsqueeze(0)\n",
    "            outputs = model_pth(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            dev_loss += round(loss.item(), 3)\n",
    "        dev_loss /= len(tars_dev)\n",
    "        dev_losses.append(dev_loss)\n",
    "\n",
    "        printProgressBar(epoch + 1, max_epoch, \n",
    "                         prefix = 'Training the model:', \n",
    "                         suffix = 'complete with loss: '+ str(dev_loss), \n",
    "                         length=50)\n",
    "    return train_losses, dev_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db45a021-9b51-4c3b-a3b5-1a731833e052",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "- In the code above, different `epoch`s are used to train the model with the training data. what does `epoch` represent and how is it different from a training `iteration`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85118b-3637-4f9d-8c77-4ad036715b31",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cf237-85dd-4787-a7b5-8a06a7321236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_pth = Model_pth(feats_train_text.shape[1])\n",
    "max_epoch = 5\n",
    "models_save_dir = \"./models\"\n",
    "\n",
    "train_losses, dev_losses = fit_pth_model(max_epoch, \n",
    "                                         model_pth, \n",
    "                                         models_save_dir,\n",
    "                                         feats_train_text, \n",
    "                                         tars_train_text,\n",
    "                                         feats_dev_text, \n",
    "                                         tars_dev_text,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164382f9-c10b-4ae9-8416-59afeb42267f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Excercise 2: Plotting loss values\n",
    "\n",
    "After the training above, `train_losses` and `dev_losses` are vectors representing the trend of losses for training and development sets for each epoch of training. Write a short script below to plot the training and development losses on the same figure in order to compare them to each other.\n",
    "\n",
    "**Note**: you can use the `matplotlib` package to draw plots with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb5392-fd98-47a0-a3f7-983d342925be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write code here to plot train and development losses per training epoch\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c2780-ba72-4d11-8bac-c6519a6a05ed",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "- How would you explain the plot above? More specifically, is there one loss that goes down and the other that goes up in relation to different epochs? If so, why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e90329-9915-4f40-960f-233173738ca0",
   "metadata": {},
   "source": [
    "## Evaluating a pytorch model\n",
    "\n",
    "Here, the code below first predicts the output of the model for the test set, and then evaluates the performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488d702-76a0-4b89-95d2-ca4a000e1457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_preds(model_pth, feats_test_text, tars_test_text):\n",
    "    model_pth.eval() # to put the model into evaluation mode\n",
    "    preds_test = []\n",
    "    for feats, tars in zip(feats_test_text, tars_test_text):\n",
    "        inputs = torch.tensor(feats).unsqueeze(0).float()\n",
    "        labels = torch.tensor(tars).unsqueeze(0)\n",
    "        outputs = model_pth(inputs).squeeze(0).detach().numpy()\n",
    "        preds_test.append(np.argmax(outputs))\n",
    "    return preds_test\n",
    "\n",
    "best_epoch = np.argmin(dev_losses)\n",
    "print(\"best epoch for development is:\", best_epoch)\n",
    "model_pth = torch.load(os.path.join(models_save_dir, f\"{best_epoch}.pth\")) # to load the best model based on development loss\n",
    "preds_test = get_preds(model_pth, feats_test_text, tars_test_text)\n",
    "preds_dev = get_preds(model_pth, feats_dev_text, tars_dev_text)\n",
    "preds_train = get_preds(model_pth, feats_train_text, tars_train_text)\n",
    "\n",
    "print(\"UAR train:\", UAR(preds_train, tars_train_text))\n",
    "print(\"UAR dev:\", UAR(preds_dev, tars_dev_text))\n",
    "print(\"UAR test:\", UAR(preds_test, tars_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6511d53c-f6f0-4a7b-8ac8-515c54e58ad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT\n",
    "\n",
    "Any given text is first tokenised and numerically encoded in order to be represented for machines. It was discussed that the simplest approaches assign independent one-hot vectors to each token (e.g. word), which would not encode any semantic or syntactic information. Statistical approaches such as TF-IDF were then introduced, which could encode the statistical saliency of each token in different documents. However, TF-IDF is a rather rudimentary statistical measure and does not take any semantics into account. On the other hand, the advent of data-driven DNNs has led to the modelling of a language using such techniques, and in many cases abandoning the traditional techniques. In fact, the DNNs can be trained in an unsupervised manner to provide us with textual feature extraction that is shaped by the data and does not depend on any specific human knowledge of the domain. This trend has become particularly popular thanks to the attention mechanism, which allows neural networks to be effectively trained on large amounts of data.\n",
    "\n",
    "The attention mechanism [see here](https://arxiv.org/abs/1706.03762) can be thought of as a machine learning method that can focus on the most important parts of the input. This is achieved by learning the weight of the input components and assigning greater importance to certain parts of the input. This allows the attention mechanism to focus on the relevant information and ignore the irrelevant parts, which can significantly improve the accuracy of sequential tasks. Although the attention mechanism can be defined in different ways, it is usually implemented as dot-product attention, and it can be mathematically written as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "    A(Q, K) = QK^T\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is the attention vector, calculated by the dot-product of $Q$ and $K$ vectors. Traditionally, the attention concept comes from retrieval systems, where a query ($Q$) is first compared to a set of keys ($K$), which are associated with a set of values ($V$). This way, the attention mechanism can map a query and a set of key-value pairs to an output, which can be computed as a weighted sum of the values. In order to compute the weighted sum of the values, a softmax function can be used on the attention vector $A$, in order to make the values describe a probability distribution. Furthermore, it is often assumed that $Q$ and $K$ vectors have $d_k$ dimensions, and that they have a random distribution with zero mean and $d_k$ variance. Thus, a common practice to divide them by $d_k$, in order for them to have zero mean and unit variance (this is known as scaling). Thus, a scaled dot-product attention can be calculated as follows:\n",
    "\\begin{equation}\n",
    "    A(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V\n",
    "\\end{equation}\n",
    "\n",
    "Moreover, in order to allow the attention mechanism to learn more complex representations of the data, multiple scaled dot-product attentions can also be used. This approach is called Multi-Head Attention, and is depicted in figure below:\n",
    "\n",
    "![MHA](./Figs/MHA.png)\n",
    "\n",
    "The advent of attention-based architectures, such as transformers, which are models consisting of multi-head attention (see figure below) and feed-forward layers, has accurate and complex language models that can better capture the context and thus have a better \"understanding\" of each input sentence as a whole. For example, `BERT` uses transformers to further improve the representation of a word by taking into account its context more effectively.\n",
    "\n",
    "![BERT](./Figs/BERT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8ddb0-adf8-4738-ab3c-3f109b47db9c",
   "metadata": {},
   "source": [
    "### Loading a BERT model from huggingface\n",
    "\n",
    "Huggingface is a development tool for machine learning methods and in particular for transformers. It is known for its large repository of different transformer models that are already trained and ready to be used for different purposes. Here we have shown how a BERT model such as [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) can be loaded into memory for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63953d06-b868-42b4-b44f-0b5ec8775d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_model = \"distilbert-base-cased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(HF_model)\n",
    "model = DistilBertModel.from_pretrained(HF_model)\n",
    "text = \"Hello there\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5ead3-7e1a-45aa-82eb-315a93333e3c",
   "metadata": {},
   "source": [
    "### Extracting BERT features\n",
    "\n",
    "Here, we pass our data through the BERT model in order to extract more contextual representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327fe9c8-dc87-4327-9889-b57d127b42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_feats(tokenizer, model, texts):\n",
    "    feats = []\n",
    "    for t, text in enumerate(texts):\n",
    "        printProgressBar(t + 1, len(texts), prefix = 'extracting bert features:', suffix = 'complete', length=50)\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)[0]\n",
    "        #output = torch.mean(output.squeeze(0), 0).detach().numpy()\n",
    "        output = output.squeeze(0).detach().numpy()\n",
    "        feats.append(output)\n",
    "    #return np.array(feats)\n",
    "    return feats\n",
    "\n",
    "feats_train_bert = extract_bert_feats(tokenizer, model, train_data[:,1])\n",
    "feats_dev_bert = extract_bert_feats(tokenizer, model, dev_data[:,1])\n",
    "feats_test_bert  = extract_bert_feats(tokenizer, model, test_data[:,1])\n",
    "\n",
    "feats_train_bert_mean = [np.concatenate((np.mean(feats[1:-1], 0), np.std(feats[1:-1], 0))) for feats in feats_train_bert]\n",
    "feats_dev_bert_mean   = [np.concatenate((np.mean(feats[1:-1], 0), np.std(feats[1:-1], 0))) for feats in feats_dev_bert]\n",
    "feats_test_bert_mean  = [np.concatenate((np.mean(feats[1:-1], 0), np.std(feats[1:-1], 0))) for feats in feats_test_bert]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795da23-de20-4a2c-b2a6-b0b38fe00c86",
   "metadata": {},
   "source": [
    "### Training an SVM model on BERT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d00f0-a6ef-42ba-90e7-9dd604e175e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_bert = SVC(C=10.0, kernel='rbf')\n",
    "clf_bert.fit(feats_train_bert_mean, tars_train_text)\n",
    "preds = clf_bert.predict(feats_test_bert_mean)\n",
    "print(\"Accuracy:\", accuracy(preds, tars_test_text))\n",
    "print(\"UAR:\", UAR(preds, tars_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d28c1-9ad4-4c8e-bd25-5fb12add6017",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "- Go back up to the loading a BERT model, and load [flaubert_large_cased](https://huggingface.co/flaubert/flaubert_large_cased) instead of the `distilbert-base-cased` (do not forget to also change the `tokenizer` and `model` as well as the `HF_model`). Then, run all the scripts again to see `flaubert_large_cased` achieves better performance than `distilbert-base-cased` in our case. Why do you think that is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac214f3-8bd6-49e8-9730-5f98e968398a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical work we started with a short summary of the previous practical work regarding loading the data and extracting `TF-IDF` measures to train `SVMs` using the `SKLearn` package. Then we were introduced to the basics of neural networks and how they can be trained using the back-propagation technique. Then we were introduced to the `PyTorch` package, which allows you to define, train and evaluate your own neural network model in a Python-friendly way. We also learned about the attention mechanism and the BERT architecture. We then learned how to use `Huggingface` to load different pre-trained `BERT` models and use them as textual feature extractors to train and evaluate machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
