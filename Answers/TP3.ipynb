{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb388c3-d254-4a1c-a1da-a2a18d564f3a",
   "metadata": {},
   "source": [
    "# Machine learning for acoustic signals and text\n",
    "\n",
    "This practical work provides an introduction to the basics of machine learning (ML) for acoustic signals and text using Python. ML refers to a set of artificial intelligence (AI) algorithms that learn to perform a task using data. The type of data used is usually referred to as the modality used for ML. Here we will be working with audio (specifically acoustic signals) and text, and the tasks considered here are sentiment classification from text, emotion classification from acoustic signals, and gender classification for acoustic signals. To perform an ML task from different input modalities, there are traditionally several stages involved, which are shown below [(HÃ¼ffmeier et al. 2020)](https://www.researchgate.net/figure/An-Overview-of-the-Steps-That-Compose-the-Machine-Learning-Process-adopted-from-13_fig4_348446831):\n",
    "\n",
    "![steps-ML](https://www.researchgate.net/profile/Johannes-Hueffmeier/publication/348446831/figure/fig4/AS:979783357825027@1610609964871/An-Overview-of-the-Steps-That-Compose-the-Machine-Learning-Process-adopted-from-13.png)\n",
    "\n",
    "\n",
    "Here, you will learn the following:\n",
    "\n",
    "- Data processing: download and load a dataset\n",
    "- Data partitions: what is the difference between different partitions and how one can define them\n",
    "- Feature extraction for text (tf-idf) and acoustics (filterbanks)\n",
    "- Defining different metrics (accuracy, and unweighted average recall)\n",
    "- Learning the basics of Support Vector Machines (SVMs), and Artificial Neural Networks (ANNs)\n",
    "- Training SVM and MultiLayer Perceptrion (MLP), which is an ANN model\n",
    "- Optimisation over different hyper-parameters, depending on the used model\n",
    "- Evaluation and analysis of the trained models\n",
    "\n",
    "Note that this practical work is divided into parts that are already filled in, which you just have to follow, and then there are two parts that need to be filled in, which are marked as `Question` or `Exercise`. In the questions you will be asked to write your answer under the question, and in the exercises you will be asked to fill in some parts of the codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7f126-39d4-4ecd-999a-e805a312f6a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Before we start with the practical work, we need to install and import the packages we will be using. To do so, run the scripts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b2103e3-8861-4da6-9bf0-7fe075a46053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from nltk==3.8.1) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from nltk==3.8.1) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from nltk==3.8.1) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from nltk==3.8.1) (2022.10.31)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.24.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (1.24.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas==1.5.3 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from pandas==1.5.3) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from pandas==1.5.3) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn==1.2.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from scikit-learn==1.2.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from scikit-learn==1.2.1) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from scikit-learn==1.2.1) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from scikit-learn==1.2.1) (1.24.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (0.2.0)\n",
      "Requirement already satisfied: future in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from ffmpeg-python==0.2.0) (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy==1.10.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from scipy==1.10.0) (1.24.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib==3.6.3 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (3.6.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (1.24.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (from matplotlib==3.6.3) (4.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.6.3) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python_speech_features==0.6.0 in /Users/sinaalisamir/Library/Python/3.9/lib/python/site-packages (0.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install beautifulsoup4==4.11.2\n",
    "!pip install nltk==3.8.1\n",
    "!pip install numpy==1.24.1\n",
    "!pip install pandas==1.5.3\n",
    "!pip install -U scikit-learn==1.2.1\n",
    "!pip install ffmpeg-python==0.2.0\n",
    "!pip install scipy==1.10.0\n",
    "!pip install matplotlib==3.6.3\n",
    "!pip install python_speech_features==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7172a360-4dc6-4413-839f-e1ed4982d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c9452-cfb2-4be7-a7d7-1eb4fff14a9c",
   "metadata": {},
   "source": [
    "## Data processing and feature extraction\n",
    "\n",
    "This section is divided into processing textual datasets, and acoustic datasets, and itroduces the processing stages and feature extraction related to each modality. In what follows, each part is explained in more detail in its related section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa158110-0c13-4c94-a816-3a341534033c",
   "metadata": {},
   "source": [
    "### Processing a textual dataset\n",
    "\n",
    "Here, we will learn how to load a textual dataset, and extract `tf-idf` features for machine learning (`tf-idf` was the subject of the previous practical work). Here, we will focus on a sentiment analysis dataset, called `AllocinÃ©`, which consists of reviews of French television series. These reviews can be positive (labelled `1`) or negative (labelled `0`). In this section, we do the following:\n",
    "\n",
    "- Download the `AllocinÃ©` corpus\n",
    "- Load the three partitions of `train`, `dev`, and `test` into memory\n",
    "- Extract `tf-idf` features for each partition\n",
    "- Read sentiment targets as numerical values, ready for machine learning usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519bd85-3d0f-4b2e-be4e-ee47880abd3a",
   "metadata": {},
   "source": [
    "#### Downloading the dataset\n",
    "\n",
    "You can run the scripts below to download the dataset. This dataset is the same as the previous practical work, so the related files can also be copied here and put under the directory `allocine` next to where this `TP.ipynb` file is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6e7890-6c87-49d6-ba5e-581364a6cb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dl_path = \"http://sentiment.nlproc.org/sentiment-dataset-fr.zip\"\n",
    "#os.system(f\"wget {dl_path}\")\n",
    "#os.system(f\"unzip ./sentiment-dataset-fr.zip -d ./allocine\")\n",
    "#os.system(f\"rm ./sentiment-dataset-fr.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f1b93-31ec-43f3-b2c2-58960c19ac17",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "\n",
    "Run the scripts below to load the dataset into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d56a9a-05eb-410c-ab03-b248b0188610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a comment in the train partition:\n",
      " probablement le meilleur pilote jamais rÃ©alisÃ© pour une sÃ©rie tÃ©lÃ© . diablement addictif et interpÃ©tÃ© de maniÃ¨re inspirÃ© , lost est une sÃ©rie Ã  ne pas manquer . \n",
      "\n",
      "An example of a comment in the dev partition:\n",
      " ca commence doucement dans les premiers Ã©pisodes mais ensuite l'histoire prend une ampleur innatendue . bons dÃ©buts Ã  confirmer dans la saison 2 . \n",
      "\n",
      "An example of a comment in the test partition:\n",
      " j'ai commencÃ© Ã  regarder la sÃ©rie Ã  ses dÃ©buts et j'avais beaucoup aimÃ© ( concept original , quelques scÃ¨nes sympathiques ) . mais depuis le dÃ©part de shannen doherty , cette sÃ©rie a perdu tout son charmed . dommage , en tout cas vivement la fin , les scenarii s'enlisent , on dÃ©croche trop rapidement . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_dataset(path):\n",
    "    train_path = os.path.join(path)\n",
    "    train_data = np.loadtxt(train_path, dtype='str', delimiter='\\t', skiprows=0)\n",
    "    return train_data\n",
    "\n",
    "# you might change the following lines according to where the files are stored in your system\n",
    "train_data = get_all_dataset(\"./allocine/fr/train.tsv\") \n",
    "dev_data = get_all_dataset(\"./allocine/fr/dev.tsv\") \n",
    "test_data = get_all_dataset(\"./allocine/fr/test.tsv\") \n",
    "\n",
    "print(\"An example of a comment in the train partition:\\n\", train_data[0,1], \"\\n\")\n",
    "print(\"An example of a comment in the dev partition:\\n\", dev_data[0,1], \"\\n\")\n",
    "print(\"An example of a comment in the test partition:\\n\", test_data[0,1], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb79d8-6d05-4e60-98bb-b1228a7ef5b5",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "- You may have noticed that the dataset above comes with predefined `train`, `dev` and `test` partitions,\n",
    "    - what is the difference between `train`, `dev`, and `test` partitions? what is the purpose of each one?\n",
    "    - Why do you think this dataset (like many others) has predefined partitions? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bba551-c0bf-4e2c-8eab-45b1db177688",
   "metadata": {},
   "source": [
    "#### Extracting features\n",
    "\n",
    "In the previous practical work, we learned about the statistical measure 'tf-idf', which can be used as a basic but useful feature for processing text. In this practical work, we would like to extract features for different partitions using `tf-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db3b68e-78fa-4ad7-b5f7-51add494ddd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first five tf-idf features for the train partition:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "the first five targets for the train partition:\n",
      " [1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def get_vectorizer(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    _ = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer\n",
    "\n",
    "corpus = train_data[:,1]\n",
    "vectorizer = get_vectorizer(corpus)\n",
    "\n",
    "feats_train_text = vectorizer.transform(train_data[:,1]).toarray()\n",
    "tars_train_text  = [int(num) for num in train_data[:,0]]\n",
    "\n",
    "feats_dev_text = vectorizer.transform(dev_data[:,1]).toarray()\n",
    "tars_dev_text  = [int(num) for num in dev_data[:,0]]\n",
    "\n",
    "feats_test_text = vectorizer.transform(test_data[:,1]).toarray()\n",
    "tars_test_text  = [int(num) for num in test_data[:,0]]\n",
    "\n",
    "print(\"the first five tf-idf features for the train partition:\\n\", feats_train_text[:5])\n",
    "print(\"the first five targets for the train partition:\\n\", tars_train_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25e360-0fb1-4c5c-937d-d0d89f3aeff3",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "- Above, what does each row in the `feats_train_text` indicate? the notion of feature is to provid distinct attributes, how can `tf-idf` values do that?\n",
    "\n",
    "- What does each item in the `tars_train_text` indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a03ed-a949-401a-914e-e9df16cf8134",
   "metadata": {},
   "source": [
    "### Processing an acoustic dataset\n",
    "\n",
    "Above, we talked about loading a textual dataset, and extracting textual features. Here, we will do the same but for an acoustic dataset. More specifically, We will work with the Canadian French Emotional (CaFE) speech dataset, which contains six different sentences, pronounced by six male and six female actors, in six basic emotions plus one neutral emotion. The six basic emotions are acted in two different intensities of mild (\"Faible\") and strong (\"Fort\") ([see here](https://zenodo.org/record/1478765#.Y_G96-zMI-Q))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c447d0b-e267-4685-9d7d-299d15ecbdc5",
   "metadata": {},
   "source": [
    "#### Downloading the dataset\n",
    "\n",
    "You can download the dataset following [this link](https://zenodo.org/record/1478765#.Y_G96-zMI-Q), or you can run the scripts below to automatically download the dataset, unzip it, and remove the zip file keeping only the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83d1680-4c92-47ca-94dd-21a1e6bac92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cmd = \"curl 'https://zenodo.org/record/1478765/files/CaFE_48k.zip?download=1' --compressed --output cafe.zip\"\n",
    "#os.system(cmd)\n",
    "#os.system(\"unzip ./cafe.zip -d ./cafe\")\n",
    "#os.system(\"rm ./cafe.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ea522-b8d2-4a13-8ad5-7efe9a75ddc7",
   "metadata": {},
   "source": [
    "#### Converting audio files (Exercise 1)\n",
    "\n",
    "It is a common practice in speech processing applications to convert all audio files to 16khz PCM encoded wav files (the reason why was asked in the previous practical work). Thus, here we want to write a function that gets the directory containing the `CaFE` dataset audio files and outputs the converted files, with the same structure in another directory.\n",
    "\n",
    "**Note**: You may use here the encoding script using `ffmpeg` for audio conversion from the previous practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55923f31-b8f4-436e-844c-136bed8b5e85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def printProgressBar (iteration: int, total: int, prefix = '', suffix = '', decimals = 1, length = \"fit\", fill = 'â') -> None:\n",
    "    \"\"\"Prints a progress bar on the terminal\n",
    "    \"\"\"\n",
    "    if length==\"fit\":\n",
    "        rows, columns = os.popen('stty size', 'r').read().split() # checks how wide the terminal width is\n",
    "        length = int(columns) // 2\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    if iteration == total: # go to new line when the progress bar is finished\n",
    "        print()\n",
    "\n",
    "def writeWavFiles(wavs_dir:str, output_dir:str, ext=\"wav\") -> None:\n",
    "    \"\"\"\n",
    "    This function writes wav files in the specific format of 16 bit integer PCM encoding at 16k rate and one audio channel (mono)\n",
    "    Inputs:\n",
    "        `wavs_dir`: the input folder of files where the unprocessed files exist\n",
    "        `output_dir`: the folder of processed wav files for the output\n",
    "    \n",
    "    Note that we would like to keep the structure of folders inside `wavs_dir` for `output_dir`, \n",
    "    for example: the file \"[wavs_dir]/Surprise/Faible/01-S-1-1.wav\" would be processed\n",
    "    and put as \"[output_dir]/Surprise/Faible/01-S-1-1.wav\"\n",
    "    \"\"\"\n",
    "    wavFiles = glob.glob(os.path.join(wavs_dir, \"**\", \"*.\"+ext), recursive=True)\n",
    "    for i, filePath in enumerate(wavFiles):\n",
    "        printProgressBar(i + 1, len(wavFiles), prefix = 'Transforming audio files:', suffix = 'complete', length=50)\n",
    "        fileDirectory = os.path.split(filePath)[0]\n",
    "        newName = os.path.split(filePath)[-1]\n",
    "        newName = os.path.splitext(newName)[0] + \".wav\"\n",
    "        # newName = newName.split(\".\")[:-1] + [\".wav\"]\n",
    "        # newName = \"\".join(newName)\n",
    "        fileNewPath = fileDirectory.replace(wavs_dir, output_dir)\n",
    "        fileNewPath = os.path.join(fileNewPath, newName)\n",
    "        #if allInOne: fileNewPath = os.path.join(output_dir, newName)\n",
    "        directory = os.path.dirname(fileNewPath)\n",
    "        if not os.path.exists(directory): os.makedirs(directory)\n",
    "        os.system('ffmpeg -i ' + filePath + ' -ar 16000 -ac 1 -c:a pcm_s16le -af \"volume=0dB\" -hide_banner -v 0 -y ' + fileNewPath)\n",
    "\n",
    "#writeWavFiles(\"./cafe\", \"./cafe_p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eca351-680b-4119-ad3e-26cad1e31ce1",
   "metadata": {},
   "source": [
    "#### Extracting features (Exercise 2)\n",
    "\n",
    "Here, the objectice is to extract useful audio features for ML. `mel filter-banks` and `mfcc` features are the two widly used traditional feature extraction techniques that are still popular today, due to their effectiveness and low computational requirements ([learn more here](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)). Such features are calculated for low periods of time (about 25 ms) where an acoustic signal is considered to be statistically stationary. Thus, you will have a different size of feature set for each audio file, with a different length. This is problematic for many machine learning approaches as the input needs to have a fixed length. This is traditionally addressed by averaging the features over time. However, averaging is only one way of statistically representing data. In order to better represent the set of acoustic features for an uttered phrase, here we would like to also use standard deviation `std`, in addition to the average `mean`. Here is a summary of your tasks for this exercise:\n",
    "\n",
    "- Here we will work with [python-speech-features](https://python-speech-features.readthedocs.io/en/latest/) package. you may use `logfbank` and `mfcc` which are already imported, and pass it as `func`, and also pass `nfilt=40` as `**kwargs` to pass the parameters to the functions. See the `python-speech-features` documentation for why and more detail.\n",
    "- Write a function that gets the directory of the processed wav files as input (see above), as well as a feature extraction function, which can be `logfbank` or `mfcc`\n",
    "- This function should calculate the mean and std of each feature set calculated for each file.\n",
    "- The output should be a dictionary that contains the basenames of the files as keys, and the features as values, for example, {\"01-N-1-1\": [2.68227271 3.66441116 2.60504873 ...], ...}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454a7dce-f918-411f-bb11-3a6fe1ced441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting features: |ââââââââââââââââââââââââââââââââââââââââââââââââââ| 100.0% complete\n",
      "extracting features: |ââââââââââââââââââââââââââââââââââââââââââââââââââ| 100.0% complete\n"
     ]
    }
   ],
   "source": [
    "def extract_features(wavs_dir:str, func:object, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    This function extracts features from audio wav files.\n",
    "    Inputs:\n",
    "        wavs_dir: the directory where the processed 16-bit and 16khz wav files exist\n",
    "        func: the function that can extract different features\n",
    "        **kwargs: to pass extra options for the `func` input\n",
    "    output:\n",
    "        a dictionary that contains the basenames of the files as keys, and the features as values\n",
    "        for example, {\"01-N-1-1\": [2.68227271 3.66441116 2.60504873 ...], ...}\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    wav_paths = glob.glob(os.path.join(wavs_dir, \"**\", \"*.wav\"), recursive=True)\n",
    "    for i, wav_path in enumerate(wav_paths):\n",
    "        printProgressBar(i + 1, len(wav_paths), prefix = 'extracting features:', suffix = 'complete', length=50)\n",
    "        (rate,sig) = wav.read(wav_path)\n",
    "        file_name = os.path.basename(wav_path).replace(\".wav\", \"\")\n",
    "        feats_all = func(sig, **kwargs)\n",
    "        means = np.mean(feats_all, 0)\n",
    "        std = np.std(feats_all, 0)\n",
    "        feats = np.concatenate((means, std))\n",
    "        output[file_name] = feats\n",
    "    return output\n",
    "\n",
    "feats_fbank = extract_features(\"./cafe_p\", logfbank, nfilt=40)\n",
    "feats_mfcc  = extract_features(\"./cafe_p\", mfcc, nfilt=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8b6fa-702c-4fb4-91b4-644818b75f45",
   "metadata": {},
   "source": [
    "#### Defining targets (Exercise 3)\n",
    "\n",
    "Now that we have extracted features, we also need to have targets, in order to train and evaluate a ML model in a supervised manner. Different datasets usually come with a little documentation about what kind of data you have and how you can use them. Here, you can go and look at the `Readme.txt` file inside the directory related to the `CaFE` dataset. As you see, you have the information related to gender, and emotional expression for each file. In the code section below, we would like to write two functions to provide us with a numerical representation of the emotion and gender information for each file. The numerical representation can be simply assigining an integer to each emotion (for example, angry=0, disgust=1, etc. or female=0, male=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b169dcb-03d8-4074-ade1-f28f8e716f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of file ids:\n",
      " 07-S-1-1\n",
      "example of emotion targets:\n",
      " 5\n",
      "example of gender targets:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "def get_emo_tar(file_names:list) -> dict:\n",
    "    \"\"\"\n",
    "    This function gets a list of file_names based on CaFE dataset, and outputs numerical targets for machine learning\n",
    "    Here, the output targets represent different emotional expressions\n",
    "    Inputs:\n",
    "        file_names: the list of file names of the CaFE dataset\n",
    "    Output:\n",
    "        a dictionary that contains the basenames of the files as keys, and the targets as values\n",
    "        for example, {\"01-N-1-1\": 3, '11-S-2-5': 5, ...}\n",
    "    \"\"\"\n",
    "    tars = {}\n",
    "    emos = [\"C\", \"D\", \"J\", \"N\", \"P\", \"S\", \"T\"]\n",
    "    for file_name in file_names:\n",
    "        tar = emos.index(file_name[3])\n",
    "        tars[file_name] = tar\n",
    "    return tars\n",
    "\n",
    "def get_gen_tar(file_names:list) -> dict:\n",
    "    \"\"\"\n",
    "    This function gets a list of file_names based on CaFE dataset, and outputs numerical targets for machine learning\n",
    "    Here, the output targets represent different genders\n",
    "    Inputs:\n",
    "        file_names: the list of file names of the CaFE dataset\n",
    "    Output:\n",
    "        a dictionary that contains the basenames of the files as keys, and the targets as values\n",
    "        for example, {\"01-N-1-1\": 1, '08-S-2-6': 0, ...}\n",
    "    \"\"\"\n",
    "    tars = {}\n",
    "    for file_name in file_names:\n",
    "        actor_num = file_name[:2]\n",
    "        rem = int(actor_num) % 2\n",
    "        tar = 0 if rem == 0 else 1\n",
    "        tars[file_name] = tar\n",
    "    return tars\n",
    "\n",
    "cafe_ids = list(feats_fbank.keys())\n",
    "emo_tars = get_emo_tar(cafe_ids)\n",
    "gender_tars  = get_gen_tar(cafe_ids)\n",
    "print(\"example of file ids:\\n\", cafe_ids[0])\n",
    "print(\"example of emotion targets:\\n\", emo_tars[cafe_ids[0]])\n",
    "print(\"example of gender targets:\\n\", gender_tars[cafe_ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e057bc-4ba6-4adf-a133-8b7473fd1de3",
   "metadata": {},
   "source": [
    "## Training and testing machine learning models\n",
    "\n",
    "Now that we have features and targets for both of our acoustic and textual datasets, its time to train and test some ML models. Here, we will work with `SVM` and `MLP` model provided by the `sklearn` Python package. We will also learn how to optimise such models using the development set, before testing them with the test set, and final evaluation of the results. The evaluation, however, requires a specific metric to be done. In what follows, you will define two metrics to be used for the evaluation of the ML models after their training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06f9b35-7e6d-4ef3-bc9d-43661c6900a6",
   "metadata": {},
   "source": [
    "### Defining metrics (Exercise 4)\n",
    "\n",
    "Here, as the task is classification, we will work with `accuracy` and `Unweighted Average Recall (UAR)`. You are most probably already familiar with accuracy, however as a metric it can be limited at times (why? is the question asked below). However, `Unweighted Average Recall` can solve some of the limitations of the accuracy metric, by calculating the average for each class first, and then taking the final average over the averages of each target class. \n",
    "\n",
    "To know more about `UAR`, you can look at: (this powerpoint)[https://ibug.doc.ic.ac.uk/media/uploads/documents/ml-lecture3-2014.pdf], and (this article)[https://ogunlao.github.io/blog/2021/04/24/consider_uar_accuracy.html], or (the sklearn metric here)[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html]. However, if you decide to use the sklearn package, be careful of its input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f98001-2304-4dfb-908f-045a0e677c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy example: 0.625\n",
      "UAR example: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "def accuracy(outs:list, tars:list) -> float:\n",
    "    \"\"\"\n",
    "    Calculating and returning the accuracy between outputs (`outs`) and targets (`tars`),\n",
    "    where each one is a list of integers like [1,0,1,2,2,1,0,3], with each integer indicating the target label\n",
    "    \"\"\"\n",
    "    accuracy = np.mean([out==tar for out, tar in zip(outs, tars)])\n",
    "    return accuracy\n",
    "\n",
    "def UAR(outs:list, tars:list) -> float:\n",
    "    \"\"\"\n",
    "    Calculating and returning the unweighted average recall between outputs (`outs`) and targets (`tars`),\n",
    "    where each one is a list of integers like [1,0,1,2,2,1,0,3], with each integer indicating the target label\n",
    "    \"\"\"\n",
    "    tarsSet = list(set(tars))\n",
    "    corrects = {}\n",
    "    totals = {}\n",
    "    for i in tarsSet:\n",
    "        corrects[i] = 0\n",
    "        totals[i] = 0\n",
    "    for i, out in enumerate(outs):\n",
    "        tar = tars[i]\n",
    "        totals[tar] += 1\n",
    "        if out == tar: corrects[tar] += 1 # Counting when target and output match per each class \n",
    "    uar = 0\n",
    "    for i in tarsSet:\n",
    "        uar += corrects[i] / totals[i] # Calculating the accuracy per each class \n",
    "    uar = uar / len(tarsSet) # Calculating the average of accuracies per each class \n",
    "    return uar\n",
    "\n",
    "outputs = [1,1,1,1,0,1,1,1]\n",
    "targets = [0,0,1,1,1,1,1,1]\n",
    "print(\"Accuracy example:\", accuracy(outputs, targets))\n",
    "print(\"UAR example:\", UAR(outputs, targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd6bd8-0060-468a-89e0-576be144b9c6",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "- Where do you think it is more useful to use UAR to evaluate a model rather than accuracy? and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f001c9-73f8-4ca3-b930-a7dae0151273",
   "metadata": {},
   "source": [
    "### Machine learning of textual features\n",
    "\n",
    "Now that we've defined our metrics, let's quickly train a model on some of our data to see what can be done and how. Different ML models can be used to map a set of features to a set of classes or labels. Here we want to use `SVC` from the `sklearn` package to do `SVM` classification. SVMs are one of the best-known ML methods, conceived about 30 years ago, but still in use today. Basic SVMs learn to separate two classes by a hyperplane in the space of features. This hyperplane separator is trained to be the optimal separator of different classes, taking into account all the training features. This hyperplane separator is considered linear in the first implementations of SVMs (see figure below), but our input features are not always linearly separable. Therefore, a mathematical kernel mechanism was soon introduced to transform the linear hyperplane into a higher dimensional space where the input features are better separable.\n",
    "\n",
    "You can ([see its wikipedia page](https://en.wikipedia.org/wiki/Support_vector_machine) and [this MIT video for more information](https://www.youtube.com/watch?v=_PwhiWxHK8o)) for more information on how `SVM`s work. \n",
    "\n",
    "![SVM-wiki](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/600px-SVM_margin.png)\n",
    "\n",
    "Run the script below to train an `SVM` classifier for a portion of training features, to have sentiment classifier. Then, predicting different sentimen classes with another portion of data, and evaluating the predictions with the two metrics of `accuracy` and `UAR`, which were discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e59537-3b78-48af-857d-e7a775687316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "UAR: 0.8133414932680538\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=10.0, kernel='linear')\n",
    "clf.fit(feats_train_text[:300], tars_train_text[:300])\n",
    "preds = clf.predict(feats_dev_text[:300])\n",
    "print(\"Accuracy:\", accuracy(preds, tars_dev_text[:300]))\n",
    "print(\"UAR:\", UAR(preds, tars_dev_text[:300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78bc1e-1153-4a0b-87b9-6a38051bbe15",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "- Try changing the kernel in the code above from `linear` to `rbf`, \n",
    "    - Which results were better? and why do you think that is? \n",
    "    - What is the difference between the `rbf` kernel and the `linear` kernel of SVM? (this question requires some research)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c82e44-a2ff-4eea-9d4c-e3fb9f15e882",
   "metadata": {},
   "source": [
    "### Hyper-parameter optimisation (Exercise 5)\n",
    "\n",
    "You can go and take a look at the documentation of the `SVC` [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). As you might notice, you have several different option to tweak in order to get better results. This can be thought of as an optimisation problem. In the code below we would like to write a function that can optimise for `C`, also referred to as SVM complexity, using the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1490d2-4734-4601-a2fe-c4ce898ee357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.5703324808184144\n",
      "0.1 0.5703324808184144\n",
      "1 0.9156010230179028\n",
      "10 0.9156010230179028\n",
      "100 0.9156010230179028\n",
      "best C is: 1\n",
      "Accuracy: 0.9002557544757033\n",
      "UAR: 0.9001831501831502\n"
     ]
    }
   ],
   "source": [
    "def get_best_model_svm(feats_train, tars_train, feats_dev, tars_dev, Cs):\n",
    "    \"\"\"\n",
    "    This function gets features, and targets for training and development sets,\n",
    "    then finds the best SVM model (`SVC`) for the parameter `C`, given a list of possible `C`s\n",
    "    Here, the output is the best_c, and also the best trained model \n",
    "    By best, we mean the model that has the best accuracy on the development set\n",
    "    Inputs:\n",
    "        feats_train: the list of training features\n",
    "        tars_train: the list of training targets\n",
    "        feats_dev: the list of development features\n",
    "        tars_dev: the list of development targets\n",
    "        Cs: the list of possible C values\n",
    "    Output:\n",
    "        best_c: the C value resulting in the best accuracy on the development set\n",
    "        best_model: the SVM model `SVC` resulting in the best accuracy on the development set\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_c = Cs[0]\n",
    "    best_model = None\n",
    "    for C in Cs:\n",
    "        clf = SVC(C=C, kernel='rbf')\n",
    "        clf.fit(feats_train, tars_train)\n",
    "        preds = clf.predict(feats_dev)\n",
    "        acc = accuracy(preds, tars_dev)\n",
    "        print(C, acc)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_c = C\n",
    "            best_model = clf\n",
    "    return best_c, best_model\n",
    "\n",
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "best_c, best_model_text = get_best_model_svm(feats_train_text, tars_train_text, feats_dev_text, tars_dev_text, Cs)\n",
    "    \n",
    "preds = best_model_text.predict(feats_test_text)\n",
    "print(\"best C is:\", best_c)\n",
    "print(\"Accuracy:\", accuracy(preds, tars_test_text))\n",
    "print(\"UAR:\", UAR(preds, tars_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67984803-44ed-4046-adb1-3259a9055059",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "- How do you think changing the value `C` for `SVC` effects training the model with calling the `fit` function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954f786-80cc-468d-aecc-d69c47bad965",
   "metadata": {},
   "source": [
    "### Testing trained model with custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa8e3c93-8bb2-48be-9878-62d42dbffc68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c'est nul Ã  chier -> Negative\n",
      "c'est pas intÃ©ressant -> Negative\n",
      "c'est trÃ¨s intÃ©ressant -> Positive\n",
      "wow c'est cool -> Positive\n",
      "je me suis bien amusÃ© avec ce TP -> Positive\n",
      "c'est pas cool Ã§a!!! -> Negative\n"
     ]
    }
   ],
   "source": [
    "custom_inputs = [\n",
    "                 \"c'est nul Ã  chier\",\n",
    "                 \"c'est pas intÃ©ressant\",\n",
    "                 \"c'est trÃ¨s intÃ©ressant\",\n",
    "                 \"wow c'est cool\",\n",
    "                 \"je me suis bien amusÃ© avec ce TP\",\n",
    "                 \"c'est pas cool Ã§a!!!\",\n",
    "                ]\n",
    "feats = vectorizer.transform(custom_inputs).toarray()\n",
    "preds = best_model_text.predict(feats)\n",
    "\n",
    "for p, pred in enumerate(preds):\n",
    "    label = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    print(custom_inputs[p], \"->\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37753c7-1e82-4f13-be24-bf7b37ba473c",
   "metadata": {},
   "source": [
    "### Machine learning of acoustic features\n",
    "\n",
    "Above, we trained an ML model for textual features, and optimised it on the development set, the same is done here, but for the acoustic features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c09710-4903-4ee6-a471-7568a24f00ef",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "The textual dataset used above, had a predefined `train`, `dev`, and `test` partitions. However, it is not always the case. For example, the `CaFE` dataset does not provide such partitioning. Thus, here we will provide a partitioning of our own, where we have about 70 percent of data as the training set, 15 percent as the development set and another 15 percent as the test set. We would also like to have these partitions balenced in terms of gender, and do not have the same speaker appear in different partitions.\n",
    "\n",
    "Run the script below to do the partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a5233-d828-4080-a280-e604b8967ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partitions(cafe_ids, train_ids=[], dev_ids=[], test_ids=[]):\n",
    "    train_keys, dev_keys, test_keys = [], [], []\n",
    "    for cafe_id in cafe_ids:\n",
    "        actor_num = cafe_id[:2]\n",
    "        if actor_num in train_ids: train_keys.append(cafe_id)\n",
    "        if actor_num in dev_ids: dev_keys.append(cafe_id)\n",
    "        if actor_num in test_ids: test_keys.append(cafe_id)\n",
    "    return train_keys, dev_keys, test_keys\n",
    "\n",
    "train_ids = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"]\n",
    "dev_ids   = [\"09\", \"10\"]\n",
    "test_ids  = [\"11\", \"12\"]\n",
    "train_keys, dev_keys, test_keys = get_partitions(cafe_ids, train_ids, dev_ids, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad684d21-34ea-4637-af9e-232fe407c948",
   "metadata": {},
   "source": [
    "### Training an MLP classifier\n",
    "\n",
    "Here, instead of the SVM, we train an `MLP` classifier. A multilayer perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN), and the term MLP often loosely means any feedforward ANN [taken from its wiki here](https://en.wikipedia.org/wiki/Multilayer_perceptron). ANNs are a type of machine learning technique, which is loosely based on the concept of biological neural networks in the human brain. Each artificial neuron, similar to the synapses and axons of a biological neuron, can be connected to other neurons to send or receive information. Artificial neurons are usually put together as groups, which are called neural layers. The most basic form of artificial neural layers, are fully connected layers, where all the neurons of the first layer is connected to all the neurons of the next layers. To describe how fully connected layers work through mathematical notations, we can consider the input of each layer to be a numerical vector, that is transformed to a different vector, through a matrix multiplication, and usually followed by a non-linear function. This process can be written as followed: \n",
    "\n",
    "$y = h(Wx + b)$\n",
    "\n",
    "Where $x$ is the input vector, $W$ is the weight matrix, $b$  is the ``bias'' vector, which is there to off-set the linear matrix multiplication, and $h(.)$ is usually a non-linear function such as tangent hyperbolic or sigmoid, and $y$ is the output vector. A fully connected layer (or an `MLP`) can be depicted as below:\n",
    "\n",
    "![MLP-fig](https://www.researchgate.net/profile/Mohamed-Zahran-16/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
    "\n",
    "Similar to `SVM`, the `sklearn` package also provides an easy to use [MLP classifier (click here for more the documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). In what follows, we will see how it can be used to train and evaluate a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d9320-c3b8-4c6a-90d0-043f682cfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feats_fbank\n",
    "tars  = gender_tars\n",
    "\n",
    "feats_train = list({key: feats[key] for key in train_keys}.values())\n",
    "tars_train  = list({key: tars [key] for key in train_keys}.values())\n",
    "feats_dev = list({key: feats[key] for key in dev_keys}.values())\n",
    "tars_dev  = list({key: tars [key] for key in dev_keys}.values())\n",
    "feats_test = list({key: feats[key] for key in test_keys}.values())\n",
    "tars_test  = list({key: tars [key] for key in test_keys}.values())\n",
    "\n",
    "#clf = SVC(C=10.0, kernel='rbf')\n",
    "clf = MLPClassifier(random_state=1, max_iter=500, hidden_layer_sizes=(256))\n",
    "clf.fit(feats_train, tars_train)\n",
    "preds = clf.predict(feats_test)\n",
    "print(\"Accuracy:\", accuracy(preds, tars_test))\n",
    "print(\"UAR:\", UAR(preds, tars_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77cdb9-5a84-4305-8988-d39aaba85708",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "- Here, the `accuracy` is the same as `UAR`, why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee496e-d134-421c-95ba-e8113cb3c0d5",
   "metadata": {},
   "source": [
    "### Hyper-parameter optimisation (Exercise 6)\n",
    "\n",
    "Similar to the optimisation of 'C' for 'SVM' above, here we want to optimise for the best number of hidden layers and nodes. Then, after writing the function below, you can choose the set of hidden layers you want to optimise for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78378488-5295-49b7-8999-721df2f2d4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_model_mlp(feats_train, tars_train, feats_dev, tars_dev, hiddens):\n",
    "    \"\"\"\n",
    "    This function gets features, and targets for training and development sets,\n",
    "    then finds the best MLP model for the hidden layer\n",
    "    Here, the output is the best hidden layer values, and also the best trained model \n",
    "    By best, we mean the model that has the best accuracy on the development set\n",
    "    Inputs:\n",
    "        feats_train: the list of training features\n",
    "        tars_train: the list of training targets\n",
    "        feats_dev: the list of development features\n",
    "        tars_dev: the list of development targets\n",
    "        hiddens: the list of possible hidden layers values\n",
    "    Output:\n",
    "        best_hidden: the hidden layers value resulting in the best accuracy on the development set\n",
    "        best_model: the MLP model resulting in the best accuracy on the development set\n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    best_hidden = hiddens[0]\n",
    "    for h, hidden in enumerate(hiddens):\n",
    "        clf = MLPClassifier(random_state=0, max_iter=500, hidden_layer_sizes=hidden)\n",
    "        clf.fit(feats_train, tars_train)\n",
    "        preds = clf.predict(feats_dev)\n",
    "        acc = accuracy(preds, tars_dev)\n",
    "        print(hidden, acc)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_hidden = hidden\n",
    "            best_model = clf\n",
    "    return best_hidden, best_model\n",
    "\n",
    "# Change the `hiddens` value to include more options for hidden layers optimisation\n",
    "hiddens = [(32), (64), (100), (128,64), (256, 128, 64)]\n",
    "best_hidden, best_model = get_best_model_mlp(feats_train, tars_train, feats_dev, tars_dev, hiddens)\n",
    "#best_model = get_best_model_svm(feats_train, tars_train, feats_dev, tars_dev, [0.1,1,10,100])\n",
    "    \n",
    "preds = best_model.predict(feats_test)\n",
    "print(\"best_hidden:\", best_hidden)\n",
    "print(\"Accuracy:\", accuracy(preds, tars_test))\n",
    "print(\"UAR:\", UAR(preds, tars_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c78fb-c184-4940-ad1f-133f7e34fe31",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "\n",
    "- Try to justify your set of choices for `hiddens`. Why did you choose that set of hidden values for optimisation?\n",
    "\n",
    "- Try changing the `get_best_model_mlp` with `get_best_model_svm`, what do you observe? (report the results here and compare them)\n",
    "\n",
    "- Go back to the previous block of code and change the target `tars` to be `emo_tars` instead of `gender_tars`. Then, run the same code above for `get_best_model_mlp` and `get_best_model_svm`, report the results here, and compare them to the results of `tars=gender_tars`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb725635-40b6-4189-b51d-5f9d65484778",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this practical work, you have learnt the basics of machine learning processes for tasks related to acoustic signals and text. You learned how to load different acoustic and textual datasets into memory, process them, and parition them if necessary, and extract effective features from them. You then learnt how to use these features in various state-of-the-art machine learning techniques to train accurate models that can predict the mood, gender and emotions of different people. You also learnt how to optimise the hyper-parameters of a machine learning model and how to evaluate them after training. The practical knowledge gained in this practical work, can be used to effectively model a large number of acoustic tasks (such as speech recognition, speaker recognition, emotion recognition, language identification) and a large number of textual tasks (such as text classification, sentiment analysis, topic modeling, and natural language processing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
